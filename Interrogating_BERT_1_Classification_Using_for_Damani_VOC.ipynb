{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Interrogating BERT 1. Classification Using for Damani-VOC).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabriwi/polyglot/blob/master/Interrogating_BERT_1_Classification_Using_for_Damani_VOC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dCpvgG0vwXAZ"
      },
      "source": [
        "# Interrogating BERT (Classification)\n",
        "\n",
        "By Peter Usherwood, \n",
        "\n",
        "Adpated from the official tutorial found here https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xiYrZKaHwV81"
      },
      "source": [
        "# 0.0 Intro\n",
        "\n",
        "BERT is currently one of (if not top of the) leaders in NLP transfer learning. To get a full picture on what BERT is I reccomend refering to the original paper https://arxiv.org/abs/1810.04805 or this excellent blog post on the NLP transfer learning zoo http://jalammar.github.io/illustrated-bert/. Here we going through a more technical user guide of how to do transfer learning with BERT. The target audience for this is data scientists who understand a bit of tensorflow/python/nlp/deep learning already.\n",
        "\n",
        "This \"tutorial\" is split into 2 notebooks, this is the first where we shall see how BERT works by looking at doing a simple classification transfer learning task. However I have dedicated a whole second notebook to understanding how to use BERT for sequence to sequence tasks (specifically question answering with a Facebook dataset) because in my opinion it is such a maaaaaaaassive pain it needs a whole extra notebook. However I wont re-cover things in here in that one so please read both :)\n",
        "\n",
        "The purpose of this notebook/tutorial is interrogate BERT to understand exactly what he is, how he works, and how we can use him. Here a lot of this notebook is based on the official tutorial for BERT, and we do use the official BERT library, however.... whenever the BERT library boilerplate is used to hide what is going on we pull it apart to understand exactly what is going on at each stage and how we 'could' modify it. The goal here is to give a comprehensive technical tutorial for transfer learning with BERT so that we can use it in the most project/data agnostic way possible, after we could go back to using boilerplate or not depending on our project. As such you will see the phrase 'BERT black box' used quite disparagingly here, however it is more used as a reference point for where were attempting to de-mistify the high level apis.\n",
        "\n",
        "Now when I say comprehensive technical tutorial, I am still treating this from the transfer learning side, I am not here diving down into how the core net works, just fully understanding the inputs, how we join the core net into a bigger model, how we train and evaluate it, and how we can use all of that in a pipeline. \n",
        "\n",
        "I built this tutorial based on the great official one found here, if you just want a quick working example I reccomend reading that instead (this one's more wordy) https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
        "\n",
        "Now not all of the high level apis here are BERTs fault, and they may not even be vexing to you. Specifically Estimators and Tensorflow Hub are used extensively here. These are not super new but new enough that they were confusing to me, as such I will lightly tackle these at the same time, particularaly in places where the BERT black box overlaps so we fully appriciate whats going on.\n",
        "\n",
        "- NOTE: Connecting to hub can be a problem on some Kantar networks, if the code in tokenizer or model hangs, take your laptop home and try there.\n",
        "- NOTE: You need Tensorflow 1.11.0> for BERT\n",
        "- NOTE: Unless you've got a nice graphics card, probably its best to upload this and the data to colab and run it there\n",
        "- NOTE: it is important to remember BERT is not a person when you read this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a-SbBG6v1HbX"
      },
      "source": [
        "# 0.1 How fat is BERT?\n",
        "\n",
        "This is a key question, it's all well and good understanding the thoery or the technical detail but if we cant run BERT on are hardware it's not much good. So how fat is he? Well unfortunately... quite. As said on the official github repo:\n",
        "\n",
        "\"Important: All results on the paper were fine-tuned on a single Cloud TPU, which has 64GB of RAM. It is currently not possible to re-produce most of the BERT-Large results on the paper using a GPU with 12GB - 16GB of RAM, because the maximum batch size that can fit in memory is too small. We are working on adding code to this repository which allows for much larger effective batch size on the GPU. See the section on out-of-memory issues for more details.\n",
        "\n",
        "This code was tested with TensorFlow 1.11.0. It was tested with Python2 and Python3 (but more thoroughly with Python2, since this is what's used internally in Google).\n",
        "\n",
        "The fine-tuning examples which use BERT-Base should be able to run on a GPU that has at least 12GB of RAM using the hyperparameters given.\"\n",
        "\n",
        "To put this in perspective most mid-range laptops with GPUs will have about 4GB VRAM (as per mine). And the best \"reasonable\" consumer graphics card you'd buy is the RTX 2080Ti which has 11GB and costs like $1200 (if you live in the US, where I am its like 2000). Now google colab do allow you to have up to I think its about 12GB VRAM so you can run BERT-small there (which is what im doing in this tutorial), but its important to bare this in mind when making producing classifiers with BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6MDYrU5k1HbY"
      },
      "source": [
        "# 0.2 How fast is BERT?\n",
        "\n",
        "So to train the base language model in BERT is quite slow, as expected its a huge transformer neural net that runs forward and back passes and does next sentence prediction. As per the published paper:\n",
        "\n",
        "\"Training of BERTBASE was performed on 4\n",
        "Cloud TPUs in Pod configuration (16 TPU chips\n",
        "total).5 Training of BERTLARGE was performed\n",
        "on 16 Cloud TPUs (64 TPU chips total). Each pretraining took 4 days to complete.\"\n",
        "\n",
        "However thats not really the focus of this tutorial (although something to bare in mind if you were thinking of training a BERT in a different language). Here we're focusing more on transfer learning using a pre-trained BERT and the good news here is it is quite fast. Training the classification example here with 1500 labeled examples only takes of the order 5-10 minutes, and this should scale linearly with number of examples (you could obviously up the number of epochs too). So in conclusion once you have the base, BERT is quite fast, if you have a graphics card that can hold him training time is not an issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ssVsHRL81HbZ"
      },
      "source": [
        "# 0.3 How good is BERT at school?\n",
        "\n",
        "Anybody thats been following NLP knows BERT got SOTAs across the board on various language tasks, one thing of particular interest is that the architecture allows for seq -> seq tasks as well as classification which makes it more felixble than some of the others (ULMFiT for example). One thing that is not fully clear to me yet is how good BERT (or indeed other transfer learning techniques) are when put under \"extreme\" conditions, for example low shot learning, dirty or noisy text data like social, how good they are going cross domain etc. Im looking into this, but thats for another time, all that is important for now is, it seems BERT is one of the best at NLP ML problems today."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2J6BFuH31Hba"
      },
      "source": [
        "# 0.4 How do we make him dance?\n",
        "\n",
        "This is the main purpose of this repo, its understanding how BERT works and how we can use it to build out useful classifiers and seq -> seq tasks. As per the official tutorial we start by looking at a classification problem (this notebook), although here we use trinary data and not binary and a different dataset (Yelp) to ensure things are agnostic as possible. After we will look at seq -> seq for contextual question answering (in the second notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb7bqR8z6fOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hsZvic2YxnTz",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub # pip install tensorflow-hub\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hhbGEfwgdEtw",
        "outputId": "f85c572e-cd4b-49cd-943f-f1f11a1c9f89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "import bert # pip install bert-tensorflow\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sX_jjO8Y1boX",
        "outputId": "12691915-1d83-45ad-95c4-efa203e86a83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Only run this if your doing this on colab, you will have to upload the data to drive too\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pmFYvkylMwXn"
      },
      "source": [
        "# 1.0 Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MC_w8SRqN0fr"
      },
      "source": [
        "First we get some data, nothing here is BERT specific, just getting any old classification data, this comes from Yelp and has been balanced and only 1, 3, and 5 star reviews are included"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fom_ff20gyy6",
        "colab": {}
      },
      "source": [
        "# df_trn = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Low Shot NLP/datasets/yelp/balanced/train.csv\")\n",
        "# df_val = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Low Shot NLP/datasets/yelp/balanced/validate.csv\")\n",
        "# df_tst = pd.read_csv(\"/content/gdrive/My Drive/Data Science/Low Shot NLP/datasets/yelp/balanced/test.csv\")\n",
        "\n",
        "# df_trn = pd.read_csv(\"./yelp/balanced/train.csv\")\n",
        "# df_val = pd.read_csv(\"./yelp/balanced/validate.csv\")\n",
        "# df_tst = pd.read_csv(\"./yelp/balanced/test.csv\")\n",
        "\n",
        "# print(df_trn[\"label\"].value_counts())\n",
        "# print(df_val[\"label\"].value_counts())\n",
        "# print(df_tst[\"label\"].value_counts())\n",
        "\n",
        "# print(df_trn['text'].dtype)\n",
        "# print(df_val['text'].dtype)\n",
        "# print(df_tst['text'].dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88Jv8paoDHgW",
        "colab_type": "text"
      },
      "source": [
        "Train on reviews, test on reviews:\n",
        "\n",
        "{'eval_accuracy': 0.71499175, 'global_step': 5593, 'loss': 0.96274084}, training takes about 45 minutes on GPU\n",
        "\n",
        "Train on twitter, test on twitter:\n",
        "\n",
        "{'eval_accuracy': 0.74233335, 'global_step': 1125, 'loss': 0.7972801}, \n",
        "31 minutes to train\n",
        "\n",
        "Train on reviews, test on twitter:\n",
        "\n",
        "{'eval_accuracy': 0.528, 'global_step': 5593, 'loss': 1.2526485}\n",
        "\n",
        "\n",
        "\n",
        "Train on twitter, test on reviews:\n",
        "\n",
        "{'eval_accuracy': 0.5837452, 'global_step': 1125, 'loss': 1.2252384}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLPs6igIGYZu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "f1a3ba23-65a2-4214-e33f-3ca6eeecd1e7"
      },
      "source": [
        "# from pathlib import Path\n",
        "# data_path = Path(\"K:/R&D/Projects/NLP PROJECT/data/english-large/\")\n",
        "# df_trn = pd.read_csv(data_path / \"train.csv\")\n",
        "# df_tst = pd.read_csv(data_path / \"test.csv\")\n",
        "# print(df_trn['is_valid'].value_counts())\n",
        "\n",
        "# df_trn = pd.read_csv(\"/content/gdrive/My Drive/DeepLearning/english-large/train.csv\")\n",
        "# df_tst = pd.read_csv(\"/content/gdrive/My Drive/DeepLearning/english-large/test.csv\")\n",
        "\n",
        "df_trn = pd.read_csv(\"/content/gdrive/My Drive/DeepLearning/english-large/tweet_train.csv\")\n",
        "df_tst = pd.read_csv(\"/content/gdrive/My Drive/DeepLearning/english-large/tweet_test.csv\")\n",
        "\n",
        "\n",
        "df_val = df_trn[df_trn['is_valid']]\n",
        "df_trn = df_trn[~df_trn['is_valid']]\n",
        "\n",
        "\n",
        "# print(df_trn[\"label\"].value_counts())\n",
        "# print(df_val[\"label\"].value_counts())\n",
        "# print(df_tst[\"label\"].value_counts())\n",
        "\n",
        "# translate [-1, 0, 1] into [1,3,5] (just in case it changes anything)\n",
        "# def translate(x):\n",
        "#     if x == 1: return 5\n",
        "#     if x == 0: return 3\n",
        "#     if x == -1: return 1\n",
        "    \n",
        "# df_trn['label'] = df_trn['label'].apply(translate)\n",
        "# df_val['label'] = df_val['label'].apply(translate)\n",
        "# df_tst['label'] = df_tst['label'].apply(translate)\n",
        "    \n",
        "print(df_trn[\"label\"].value_counts())\n",
        "print(df_val[\"label\"].value_counts())\n",
        "print(df_tst[\"label\"].value_counts())\n",
        "\n",
        "print(df_trn['text'].dtype)\n",
        "print(df_val['text'].dtype)\n",
        "print(df_tst['text'].dtype)\n",
        "\n",
        "\n",
        "df_trn['text'] = df_trn['text'].astype('str')\n",
        "df_val['text'] = df_val['text'].astype('str')\n",
        "df_tst['text'] = df_tst['text'].astype('str')\n",
        "\n",
        "\n",
        "\n",
        "# Sampling down to yelp numbers and balancing at the same time:\n",
        "# each class has these number of instances 524 train, 75 validate, 150 test\n",
        "\n",
        "# sizes:\n",
        "train_size = 524\n",
        "val_size = 75\n",
        "test_size = 150\n",
        "# {'eval_accuracy': 0.62666667, 'global_step': 147, 'loss': 1.1610295}, training takes about 2 minutes on GPU\n",
        "\n",
        "# min size to keep in balanced:\n",
        "# train: 19887, val: 4093, test: 607\n",
        "# {'eval_accuracy': 0.71499175, 'global_step': 5593, 'loss': 0.96274084}, training takes about 45 minutes on GPU\n",
        "\n",
        "g = df_trn.groupby('label')\n",
        "# df_trn = g.apply(lambda x: x.sample(n=train_size, random_state=1).reset_index(drop=True))\n",
        "df_trn = g.apply(lambda x: x.sample(n=g.size().min(), random_state=1).reset_index(drop=True))\n",
        "\n",
        "g = df_val.groupby('label')\n",
        "# df_val = g.apply(lambda x: x.sample(n=val_size, random_state=1).reset_index(drop=True))\n",
        "df_val = g.apply(lambda x: x.sample(n=g.size().min(), random_state=1).reset_index(drop=True))\n",
        "\n",
        "g = df_tst.groupby('label')\n",
        "# df_tst = g.apply(lambda x: x.sample(n=test_size, random_state=1).reset_index(drop=True))\n",
        "df_tst = g.apply(lambda x: x.sample(n=g.size().min(), random_state=1).reset_index(drop=True))\n",
        "\n",
        "print(df_trn[\"label\"].value_counts())\n",
        "print(df_val[\"label\"].value_counts())\n",
        "print(df_tst[\"label\"].value_counts())\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1    4000\n",
            " 1    4000\n",
            " 0    4000\n",
            "Name: label, dtype: int64\n",
            " 1    500\n",
            " 0    500\n",
            "-1    500\n",
            "Name: label, dtype: int64\n",
            " 1    1000\n",
            "-1    1000\n",
            " 0    1000\n",
            "Name: label, dtype: int64\n",
            "object\n",
            "object\n",
            "object\n",
            "-1    4000\n",
            " 1    4000\n",
            " 0    4000\n",
            "Name: label, dtype: int64\n",
            " 1    500\n",
            " 0    500\n",
            "-1    500\n",
            "Name: label, dtype: int64\n",
            " 1    1000\n",
            "-1    1000\n",
            " 0    1000\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XA8WHJgzhIZf"
      },
      "source": [
        "Make sure we shuffle the data here or BERT fails"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "prRQM8pDi8xI",
        "outputId": "3fb2fbfa-1697-4516-8022-e6c63f623955",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "df_trn = df_trn[[\"text\", \"label\"]]\n",
        "df_tst = df_tst[[\"text\", \"label\"]]\n",
        "\n",
        "df_trn = df_trn.sample(frac=1)\n",
        "df_tst = df_tst.sample(frac=1)\n",
        "\n",
        "df_trn.sample(n=10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <th>2814</th>\n",
              "      <td>Don't miss the beast Brock Lesnar on ESPN tomo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
              "      <th>3264</th>\n",
              "      <td>Ellen returns with new season Tuesday, intervi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3449</th>\n",
              "      <td>Check Out #Omen from @disclosure ft. Sam Smith...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>-1</th>\n",
              "      <th>909</th>\n",
              "      <td>Dustin Johnson just can't do Sunday's</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
              "      <th>435</th>\n",
              "      <td>Good driving/singing tunes The Bee Gees, might...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1848</th>\n",
              "      <td>I feel like Dustin Johnson always  has the lea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
              "      <th>136</th>\n",
              "      <td>@Giggles5050 @DrGarekar I think Patels along w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2722</th>\n",
              "      <td>Lady Gaga appeared to momentarily regret her c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2952</th>\n",
              "      <td>Part of me thinks Nicki is so serious about he...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2430</th>\n",
              "      <td>Remember this when you watch the news stories ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         text  label\n",
              "label                                                               \n",
              " 1    2814  Don't miss the beast Brock Lesnar on ESPN tomo...      1\n",
              " 0    3264  Ellen returns with new season Tuesday, intervi...      0\n",
              "      3449  Check Out #Omen from @disclosure ft. Sam Smith...      0\n",
              "-1    909               Dustin Johnson just can't do Sunday's     -1\n",
              " 1    435   Good driving/singing tunes The Bee Gees, might...      1\n",
              "      1848  I feel like Dustin Johnson always  has the lea...      1\n",
              " 0    136   @Giggles5050 @DrGarekar I think Patels along w...      0\n",
              "      2722  Lady Gaga appeared to momentarily regret her c...      0\n",
              "      2952  Part of me thinks Nicki is so serious about he...      0\n",
              "      2430  Remember this when you watch the news stories ...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sfRnHSz3iSXz"
      },
      "source": [
        "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IuMOGwFui4it",
        "colab": {}
      },
      "source": [
        "DATA_COLUMN = 'text'\n",
        "LABEL_COLUMN = 'label'\n",
        "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
        "# label_list = [1, 3, 5]\n",
        "label_list = [-1, 0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V399W0rqNJ-Z"
      },
      "source": [
        "# 2.0 Data Preprocessing\n",
        "We'll need to transform our data into a format BERT understands. This involves three steps:\n",
        "- First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
        "- Second, we load the model specific tokenizer\n",
        "- Third, we convert examples to features BERT understands using parts 1 and 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sBhP3uyY1Hbt"
      },
      "source": [
        "# 2.1 Input Features\n",
        "\n",
        "These classes are super simple and litterally just have 4 attributes that hold the text. What info they hold is listed below.\n",
        "\n",
        "\n",
        "- `text_a` is the text we want to classify, which in this case, is the `text` field in our Dataframe. \n",
        "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
        "- `label` is the label for our example, i.e. True, False"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p9gEt5SmM6i6",
        "colab": {}
      },
      "source": [
        "train_InputExamples = df_trn.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_InputExamples = df_tst.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ktv2USB91Hbx"
      },
      "source": [
        "## Understanding what BERT eats\n",
        "\n",
        "Now this is an ok explination to operate BERT basically, but if we want to really understand what is going inside we need to fully approciate theoretically what BERT wants for its inputs (this part is key for lots of later discussion to). And BERTs input representation is not intuitive (in my opinion).\n",
        "\n",
        "First we can mention how it wants its inputs ordered. \n",
        "\n",
        "It can take up to 512 tokens in a single 'sequence'. A sequence is defined as one input example (however that may be constructed). Note this limit on 512 is actually very important when we come to look at question answering.\n",
        "\n",
        "Said sequence is composed in one of two ways as best described by comments in the oficial repo...\n",
        "\n",
        "The convention in BERT is:\n",
        "\n",
        "(a) For sequence pairs:\n",
        "\n",
        "tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "\n",
        "(b) For single sequences:\n",
        "\n",
        "tokens:   [CLS] the dog is hairy . [SEP]\n",
        "\n",
        "So depending on if we're doing single sequence or sequence to sequence we arrange the data in one of these two ways. Note BERT refer to the whole input as the sequence, and each of the two potential parts as sentences even though they can be longer or shorter than a linguistic sentence.\n",
        "\n",
        "Next we have to understand what BERT does with these words, goes to word embeddings? We'll yes but not so simply...\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "BERT actually sums 3 different embeddings, however as we shall see we only actually need to create the ids for the word embeddings and the segment embeddings as inputs to the model. Creating the positional embeddings, gettings the vectors, and summing is all done internally, and in this notebook we're not messing with core BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SCZWZtKxObjh"
      },
      "source": [
        "# 2.1 Tokenizer\n",
        "\n",
        "Next, we need to preprocess our data so that it matches the data BERT was trained on.This follows the following process.\n",
        "\n",
        "\n",
        "1. Lowercase our text (if we're using a BERT lowercase model)\n",
        "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
        "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
        "4. Map our words to indexes using a vocab file that BERT provides\n",
        "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
        "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "Now the original blog says \"Happily, we don't have to worry about most of these details.\", and simply instanciates an instance of the bert.tokenization.FullTokenizer class... but I want to go deeper.\n",
        "\n",
        "So lets look at BERTs tokenizer setup:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6D1lYz8q1Hbz"
      },
      "source": [
        "```\n",
        "######################### SOURCE CODE ###############################\n",
        "class FullTokenizer(object):\n",
        "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_file, do_lower_case=True):\n",
        "    self.vocab = load_vocab(vocab_file)\n",
        "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    split_tokens = []\n",
        "    for token in self.basic_tokenizer.tokenize(text):\n",
        "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "        split_tokens.append(sub_token)\n",
        "\n",
        "    return split_tokens\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    return convert_by_vocab(self.vocab, tokens)\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids):\n",
        "    return convert_by_vocab(self.inv_vocab, ids)\n",
        "######################### SOURCE CODE ###############################\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qMWiDtpyQSoU"
      },
      "source": [
        "This above code is taken directly from their repository, as we can see the tokenizer has all the basic components we'd expect. \n",
        "\n",
        "- vocab is a dictionary that maps ids to words in the input vocab (familiar to anyone acustomed to working with language models)\n",
        "- inv_vocab is the oposite, words to ids\n",
        "- they then have two tokenizers, one is a more basic form and the other splits out word parts, they could have grouped the two\n",
        "\n",
        "Now this is simple enoguh, however the \"recomended\" way of instanciating this is (to me) super confusing!!! (See below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IhJSe0QHNG7U",
        "outputId": "16c3b3d7-69a5-4e91-9408-68ef2111eca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "\n",
        "\"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "with tf.Graph().as_default():\n",
        "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    with tf.Session() as sess:\n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                              tokenization_info[\"do_lower_case\"]])\n",
        "        \n",
        "tokenizer = bert.tokenization.FullTokenizer(vocab_file=vocab_file, \n",
        "                                            do_lower_case=do_lower_case)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J1QAGT-K1Hb3"
      },
      "source": [
        "So... what happened?\n",
        "\n",
        "Well here we are using Tensorflow Hub, which is Tensorflows newest way of easily sharing pre-trained models, its vital to understand this for anyone doing transfer learning in TF. The BERT model was loaded in the deafult graph (standard Tensorflow) from Hub https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1 by calling a set filepath into hub.Module. As such to go any further we really need to understand exactly what the hub.Module returns, and how we get at the underlying module. To start the official docs https://www.tensorflow.org/hub/api_docs/python/hub/Module are actually quite good on Module.__init__:\n",
        "\n",
        "######################### OFFICIAL DOCS ###############################\n",
        "\n",
        "Constructs a Module to be used in the current graph.\n",
        "\n",
        "This creates the module state-graph under an unused variable_scope based on name. During this call a Module will:\n",
        "\n",
        "Add GLOBAL_VARIABLES under its scope. Those variables may be added to to the TRAINABLE_VARIABLES collection (depending on trainable parameter) and to the MODEL_VARIABLES. The variables must be initialized before use, and can be checkpointed as usual.\n",
        "\n",
        "Add ops to the INIT_TABLE_OPS collection, which must be run during session initialization and add constant tensors to ASSET_FILEPATHS that are needed during the execution of such ops.\n",
        "\n",
        "Add tensors to the REGULARIZATION_LOSSES collection (depending on trainable parameter).\n",
        "\n",
        "Args:\n",
        "- spec: A ModuleSpec defining the Module to instantiate or a path where to load a ModuleSpec from via load_module_spec.\n",
        "- trainable: whether the Module is trainable. If False, no variables are added to TRAINABLE_VARIABLES collection, and no tensors are added to REGULARIZATION_LOSSES collection.\n",
        "- name: A string, the variable scope name under which to create the Module. It will be uniquified and the equivalent name scope must be unused.\n",
        "- tags: A set of strings specifying the graph variant to use.\n",
        "\n",
        "######################### OFFICIAL DOCS ###############################\n",
        "\n",
        "Thats reasonably clear (for tensorflow) we now have all of the variables of the BERT graph defined under a specific variable scope. It also added these to some dictionaries of what variables exist (for example trainable parameters). Finally it adds the operations to the INIT_TABLE_OPS.\n",
        "\n",
        "So now I have at least 3 questions (maybe inquisitive you has many more :))... \n",
        "- How do we access the variables? Can we modify the graph? \n",
        "- How do we call it? In what way is it being called?\n",
        "- What in the world are these next 3 lines!?!?\n",
        "\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "    \n",
        "        vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                            tokenization_info[\"do_lower_case\"]])\n",
        "                                            \n",
        "To help with these im drawing a lot of inspiration from this tutorial that you should definetly check out https://medium.com/ymedialabs-innovation/how-to-use-tensorflow-hub-with-code-examples-9100edec29af.\n",
        "\n",
        "So to start with once we have the instance it is called using a 'signature'. Whats a signature? Well the signatures are pre-defined use cases for how the creator wants the user to call their model. We can see the available signatures by running:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i0cYqT871Hb3",
        "outputId": "86a5ffc1-75a6-4a3b-b380-0fedd7c59b78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bert_module.get_signature_names()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mlm', 'tokens', 'tokenization_info']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SiL6zt_Y1Hb6"
      },
      "source": [
        "Well now we have these abstracted use cases, but what do we need to give to them? And what do they output? As is the case with BERT this is documented on tf hub, they only actually support the 'tokens' signature... (even though we're using the tokenization_info one here...), however we can check by running:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FNWxM2rv1Hb7",
        "outputId": "009652dd-560d-4fa1-82b5-3f79f67a143b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "print(bert_module.get_input_info_dict(signature='tokens'))\n",
        "print(bert_module.get_output_info_dict(signature='tokens'))\n",
        "\n",
        "print(bert_module.get_input_info_dict(signature='tokenization_info'))\n",
        "print(bert_module.get_output_info_dict(signature='tokenization_info'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'segment_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'input_mask': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'input_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>}\n",
            "{'sequence_output': <hub.ParsedTensorInfo shape=(?, ?, 768) dtype=float32 is_sparse=False>, 'pooled_output': <hub.ParsedTensorInfo shape=(?, 768) dtype=float32 is_sparse=False>}\n",
            "{}\n",
            "{'do_lower_case': <hub.ParsedTensorInfo shape=() dtype=bool is_sparse=False>, 'vocab_file': <hub.ParsedTensorInfo shape=() dtype=string is_sparse=False>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TCEPnSMg1Hb9"
      },
      "source": [
        "So given what we know about what BERT eats and the output we'd expect from a language model, 'tokens' seems to make sense in what it does, that is the main running process. However thats section 3. Here lets look more at 'tokenization_info', 'tokenization_info' doesnt take any inputs (makes sense as the tokenization should depend only on the model and not on the inputs), and gives two outputs: the vocab file and if we should lower everything... makes sense.\n",
        "\n",
        "Now this business makes sense:\n",
        "```\n",
        "tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "with tf.Session() as sess:\n",
        "    vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                        tokenization_info[\"do_lower_case\"]])\n",
        "```                                           \n",
        "As tokenization_info (the variable) here is a pointer to two tensors in the graph, each of which are called and their output returned, noramlly in session.run we'd pass the input as well but we already know for these specific endpoints there are no inputs. Now session.run will run the pretrained graph and these two endpoints will give us what we need.\n",
        "\n",
        "I havn't addressed the accessing and messing with the main graph yet, I think that makes more sense in the model section where as it turns out we need to reload the model from tf hub in the same way, so for now we'll continue just knowning how to call the model and what it returns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z4oFkhpZBDKm"
      },
      "source": [
        "So back to the original tutorial... \"we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsBo6RCtQmwx",
        "outputId": "7665be77-382e-47cb-fd7f-8c45bb7670b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'here',\n",
              " \"'\",\n",
              " 's',\n",
              " 'an',\n",
              " 'example',\n",
              " 'of',\n",
              " 'using',\n",
              " 'the',\n",
              " 'bert',\n",
              " 'token',\n",
              " '##izer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jyMW3Cg41HcC"
      },
      "source": [
        "Now to add a bit of \"whats going on\" we can easily parse the vocab attribute (that we saw earlier as part of the tokenizer) to a pandas df to view the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nFINp5oN1HcD",
        "outputId": "fa25a4a4-62f3-41b4-fbd7-cfa1d4255d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "vocab_df = pd.DataFrame(np.array([list(tokenizer.vocab.keys()), list(tokenizer.vocab.values())]).T, columns=[\"Vocab\", \"ID\"])\n",
        "vocab_df.sample(n=10)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Vocab</th>\n",
              "      <th>ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11173</th>\n",
              "      <td>iris</td>\n",
              "      <td>11173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9545</th>\n",
              "      <td>fountain</td>\n",
              "      <td>9545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3479</th>\n",
              "      <td>selected</td>\n",
              "      <td>3479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>[unused117]</td>\n",
              "      <td>122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10046</th>\n",
              "      <td>glacier</td>\n",
              "      <td>10046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11586</th>\n",
              "      <td>din</td>\n",
              "      <td>11586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11848</th>\n",
              "      <td>toilet</td>\n",
              "      <td>11848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13698</th>\n",
              "      <td>alps</td>\n",
              "      <td>13698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4516</th>\n",
              "      <td>documentary</td>\n",
              "      <td>4516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>952</th>\n",
              "      <td>[unused947]</td>\n",
              "      <td>952</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Vocab     ID\n",
              "11173         iris  11173\n",
              "9545      fountain   9545\n",
              "3479      selected   3479\n",
              "122    [unused117]    122\n",
              "10046      glacier  10046\n",
              "11586          din  11586\n",
              "11848       toilet  11848\n",
              "13698         alps  13698\n",
              "4516   documentary   4516\n",
              "952    [unused947]    952"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x2lldTi11HcG",
        "outputId": "ad8c6f5f-69e0-4366-f3dd-4eeda40e5521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(bert_module.variable_map))\n",
        "tokenization_info['do_lower_case']"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'module_apply_tokenization_info/Const:0' shape=() dtype=bool>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0OEzfFIt6GIc"
      },
      "source": [
        "# 2.3 Examples to Features\n",
        "\n",
        "Next.... Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands. Now we know that BERT is hoping for inputs of the form:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QG3YCaqF1HcJ",
        "outputId": "cbda2cb4-aade-4ed6-998c-a60475a584bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(bert_module.get_input_info_dict(signature='tokens'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'segment_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'input_mask': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'input_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eGZrLqdm1HcO"
      },
      "source": [
        "But as is this is completely black box as to what is happening here! Let's look"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LL5W8gEGRTAf",
        "outputId": "3dce7164-9559-4ded-a1c9-79eecc6ee997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We'll set sequences to be at most 128 tokens long.\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Convert our train and test features to InputFeatures that BERT understands.\n",
        "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 12000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 12000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] tonight going to see foo fighters in concert + friday ken ##ney che ##s ##ney : ) ) [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] tonight going to see foo fighters in concert + friday ken ##ney che ##s ##ney : ) ) [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3892 2183 2000 2156 29379 7299 1999 4164 1009 5958 6358 5420 18178 2015 5420 1024 1007 1007 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3892 2183 2000 2156 29379 7299 1999 4164 1009 5958 6358 5420 18178 2015 5420 1024 1007 1007 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] w ##th , only hulk hogan blocked me during my # celebrity ##sl ##ander ##sat ##ur ##day ? i guess it ' s still saturday . round two ! [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] w ##th , only hulk hogan blocked me during my # celebrity ##sl ##ander ##sat ##ur ##day ? i guess it ' s still saturday . round two ! [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1059 2705 1010 2069 16009 14851 8534 2033 2076 2026 1001 8958 14540 12243 16846 3126 10259 1029 1045 3984 2009 1005 1055 2145 5095 1012 2461 2048 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1059 2705 1010 2069 16009 14851 8534 2033 2076 2026 1001 8958 14540 12243 16846 3126 10259 1029 1045 3984 2009 1005 1055 2145 5095 1012 2461 2048 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i ' m honestly really satisfied with my classes except 3rd but it ' s gu ##cci [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i ' m honestly really satisfied with my classes except 3rd but it ' s gu ##cci [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 1005 1049 9826 2428 8510 2007 2026 4280 3272 3822 2021 2009 1005 1055 19739 14693 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 1005 1049 9826 2428 8510 2007 2026 4280 3272 3822 2021 2009 1005 1055 19739 14693 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] fuck that next fight floyd may ##weather got next saturday night . it ' s going to be weak as hell [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] fuck that next fight floyd may ##weather got next saturday night . it ' s going to be weak as hell [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6616 2008 2279 2954 12305 2089 28949 2288 2279 5095 2305 1012 2009 1005 1055 2183 2000 2022 5410 2004 3109 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6616 2008 2279 2954 12305 2089 28949 2288 2279 5095 2305 1012 2009 1005 1055 2183 2000 2022 5410 2004 3109 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: -1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: -1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] kamal ##a , who defended hulk hogan following the racial ti ##rade scandal that broke out on friday , discussed the . . . http : / / t . co / 9 ##c ##j ##x ##9 ##n ##wo ##5 ##o [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] kamal ##a , who defended hulk hogan following the racial ti ##rade scandal that broke out on friday , discussed the . . . http : / / t . co / 9 ##c ##j ##x ##9 ##n ##wo ##5 ##o [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 21911 2050 1010 2040 8047 16009 14851 2206 1996 5762 14841 13662 9446 2008 3631 2041 2006 5958 1010 6936 1996 1012 1012 1012 8299 1024 1013 1013 1056 1012 2522 1013 1023 2278 3501 2595 2683 2078 12155 2629 2080 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 21911 2050 1010 2040 8047 16009 14851 2206 1996 5762 14841 13662 9446 2008 3631 2041 2006 5958 1010 6936 1996 1012 1012 1012 8299 1024 1013 1013 1056 1012 2522 1013 1023 2278 3501 2595 2683 2078 12155 2629 2080 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: -1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: -1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 10000 of 12000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 10000 of 12000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 3000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] # bono hand delivered pizza to fans waiting in ga line for the u2 concert tomorrow . this band is the definition of awesome . # u2 # u2 ##ie ##tour [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] # bono hand delivered pizza to fans waiting in ga line for the u2 concert tomorrow . this band is the definition of awesome . # u2 # u2 ##ie ##tour [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1001 23648 2192 5359 10733 2000 4599 3403 1999 11721 2240 2005 1996 23343 4164 4826 1012 2023 2316 2003 1996 6210 1997 12476 1012 1001 23343 1001 23343 2666 21163 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1001 23648 2192 5359 10733 2000 4599 3403 1999 11721 2240 2005 1996 23343 4164 4826 1012 2023 2316 2003 1996 6210 1997 12476 1012 1001 23343 1001 23343 2666 21163 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] josh hamilton 2 run blast ! rangers lead the giants 2 - 0 in the 6th ! # never ##ever ##qui ##t [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] josh hamilton 2 run blast ! rangers lead the giants 2 - 0 in the 6th ! # never ##ever ##qui ##t [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6498 5226 1016 2448 8479 999 7181 2599 1996 7230 1016 1011 1014 1999 1996 5351 999 1001 2196 22507 15549 2102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 6498 5226 1016 2448 8479 999 7181 2599 1996 7230 1016 1011 1014 1999 1996 5351 999 1001 2196 22507 15549 2102 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i wish i could buy something but he doesn ' t ship to israel . . . https : / / t . co / hd ##df ##8 ##8 ##uo ##c ##j [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] i wish i could buy something but he doesn ' t ship to israel . . . https : / / t . co / hd ##df ##8 ##8 ##uo ##c ##j [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 4299 1045 2071 4965 2242 2021 2002 2987 1005 1056 2911 2000 3956 1012 1012 1012 16770 1024 1013 1013 1056 1012 2522 1013 10751 20952 2620 2620 19098 2278 3501 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1045 4299 1045 2071 4965 2242 2021 2002 2987 1005 1056 2911 2000 3956 1012 1012 1012 16770 1024 1013 1013 1056 1012 2522 1013 10751 20952 2620 2620 19098 2278 3501 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 0 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] hey , did you know national ice cream day is sunday , july 19th ? make sure you take a moment to enjoy a scoop of your favorite flavor ! # yu ##m [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] hey , did you know national ice cream day is sunday , july 19th ? make sure you take a moment to enjoy a scoop of your favorite flavor ! # yu ##m [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4931 1010 2106 2017 2113 2120 3256 6949 2154 2003 4465 1010 2251 3708 1029 2191 2469 2017 2202 1037 2617 2000 5959 1037 23348 1997 2115 5440 14894 999 1001 9805 2213 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 4931 1010 2106 2017 2113 2120 3256 6949 2154 2003 4465 1010 2251 3708 1029 2191 2469 2017 2202 1037 2617 2000 5959 1037 23348 1997 2115 5440 14894 999 1001 9805 2213 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 2)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] @ matt ##fo ##ssen @ frank ##con ##ni ##ff 1st , they did do plan 9 . 2nd , shark ##nad ##o is not earnest ##ly bad , it ' s ironically so . that ' s why ms ##t ##3 ##k wouldn ' t work [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] @ matt ##fo ##ssen @ frank ##con ##ni ##ff 1st , they did do plan 9 . 2nd , shark ##nad ##o is not earnest ##ly bad , it ' s ironically so . that ' s why ms ##t ##3 ##k wouldn ' t work [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1030 4717 14876 14416 1030 3581 8663 3490 4246 3083 1010 2027 2106 2079 2933 1023 1012 3416 1010 11420 25389 2080 2003 2025 17300 2135 2919 1010 2009 1005 1055 18527 2061 1012 2008 1005 1055 2339 5796 2102 2509 2243 2876 1005 1056 2147 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1030 4717 14876 14416 1030 3581 8663 3490 4246 3083 1010 2027 2106 2079 2933 1023 1012 3416 1010 11420 25389 2080 2003 2025 17300 2135 2919 1010 2009 1005 1055 18527 2061 1012 2008 1005 1055 2339 5796 2102 2509 2243 2876 1005 1056 2147 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: -1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: -1 (id = 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nWuSCvDf2RDb"
      },
      "source": [
        "Right.... lets have a look at exactly what happened there by again looking at the source code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kFXtaWJZ1HcT"
      },
      "source": [
        "######################### SOURCE CODE ###############################\n",
        "```\n",
        "# This function is not used by this file but is still used by the Colab and\n",
        "# people who depend on it.\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
        "                                 tokenizer):\n",
        "  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "  features = []\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "    if ex_index % 10000 == 0:\n",
        "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "    feature = convert_single_example(ex_index, example, label_list,\n",
        "                                     max_seq_length, tokenizer)\n",
        "\n",
        "    features.append(feature)\n",
        "  return features\n",
        "```\n",
        "######################### SOURCE CODE ###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IrYccyBP1HcU"
      },
      "source": [
        "So lets have a look whats happening here...\n",
        "Inputs:\n",
        "- examples - it is taking as input our list of classes as \"examples\", where each class just has 4 arributes that hold the relevant info (guid, text_a, text_b, label)\n",
        "- label_list - our python list with the values of the labels for our classification task\n",
        "- max_sequence_length - Integer, a hyperparameter of how many steps (word parts) we want to analyse, common hyperparameter for those of us familiar with language modelling\n",
        "- tokenizer -  the custom tokenizer class that holds the vocab and methods for tokenizing text.\n",
        "\n",
        "So whats the method doing? Well its simply looping through each example (doing a bit of logging) and then appending a simple converted example to a python list before returning the list. This single example depends on all the things above but now also has an index. Lets look at the source code for convert_single_example..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tA_JNt5o1HcU"
      },
      "source": [
        "######################### SOURCE CODE ###############################\n",
        "```\n",
        "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
        "                           tokenizer):\n",
        "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "  if isinstance(example, PaddingInputExample):\n",
        "    return InputFeatures(\n",
        "        input_ids=[0] * max_seq_length,\n",
        "        input_mask=[0] * max_seq_length,\n",
        "        segment_ids=[0] * max_seq_length,\n",
        "        label_id=0,\n",
        "        is_real_example=False)\n",
        "\n",
        "  label_map = {}\n",
        "  for (i, label) in enumerate(label_list):\n",
        "    label_map[label] = i\n",
        "\n",
        "  tokens_a = tokenizer.tokenize(example.text_a)\n",
        "  tokens_b = None\n",
        "  if example.text_b:\n",
        "    tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "  if tokens_b:\n",
        "    # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "    # length is less than the specified length.\n",
        "    # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "  else:\n",
        "    # Account for [CLS] and [SEP] with \"- 2\"\n",
        "    if len(tokens_a) > max_seq_length - 2:\n",
        "      tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
        "\n",
        "  # The convention in BERT is:\n",
        "  # (a) For sequence pairs:\n",
        "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "  # (b) For single sequences:\n",
        "  #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "  #  type_ids: 0     0   0   0  0     0 0\n",
        "  #\n",
        "  # Where \"type_ids\" are used to indicate whether this is the first\n",
        "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "  # embedding vector (and position vector). This is not *strictly* necessary\n",
        "  # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "  # it easier for the model to learn the concept of sequences.\n",
        "  #\n",
        "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "  # used as the \"sentence vector\". Note that this only makes sense because\n",
        "  # the entire model is fine-tuned.\n",
        "  tokens = []\n",
        "  segment_ids = []\n",
        "  tokens.append(\"[CLS]\")\n",
        "  segment_ids.append(0)\n",
        "  for token in tokens_a:\n",
        "    tokens.append(token)\n",
        "    segment_ids.append(0)\n",
        "  tokens.append(\"[SEP]\")\n",
        "  segment_ids.append(0)\n",
        "\n",
        "  if tokens_b:\n",
        "    for token in tokens_b:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(1)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(1)\n",
        "\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "  # tokens are attended to.\n",
        "  input_mask = [1] * len(input_ids)\n",
        "\n",
        "  # Zero-pad up to the sequence length.\n",
        "  while len(input_ids) < max_seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "\n",
        "  assert len(input_ids) == max_seq_length\n",
        "  assert len(input_mask) == max_seq_length\n",
        "  assert len(segment_ids) == max_seq_length\n",
        "\n",
        "  label_id = label_map[example.label]\n",
        "  if ex_index < 5:\n",
        "    tf.logging.info(\"*** Example ***\")\n",
        "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
        "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "        [tokenization.printable_text(x) for x in tokens]))\n",
        "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "  feature = InputFeatures(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids,\n",
        "      label_id=label_id,\n",
        "      is_real_example=True)\n",
        "  return feature\n",
        "```\n",
        "######################### SOURCE CODE ###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a-YwzZ3-1HcV"
      },
      "source": [
        "This is a bit more involved (i.e. long) but its actually quite transparent what they're doing if we break it down. So lets start by remembering what we mentioned earlier around how the BERT model want to read its inputs. BERT wants inputs in the form:\n",
        "\n",
        "(a) For sequence pairs:\n",
        "\n",
        "tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "\n",
        "(b) For single sequences:\n",
        "\n",
        "tokens:   [CLS] the dog is hairy . [SEP]\n",
        "\n",
        "So thats what the code above is doing:\n",
        "1. It has some house keeping to check for fake examples (we'll ignore that for now)\n",
        "2. It tokenizes text_a and if it exists text_b\n",
        "3. Depending on if text_b exists it limits the combined 'sequence' to the max_sequence_length\n",
        "4. Then it creates 2 lists\n",
        "    - The list of tokens with special tokens [SEP] etc\n",
        "    - The list of type ids which is 0 for text_a and 0 for text_b\n",
        "5. Next it converts the tokenized text to ids (which the language model needs to run)\n",
        "6. Then it creates a new list, and input mask so it knows which of the tokens out of max_sequence_length are actual data and not just fillers\n",
        "7. It pads our 3 lists: word_ids, type_ids, and input_mask; out to max_sequence_length with 0s\n",
        "8. Then it passes these 3 lists with the label_id (which it made a mapping of earlier) to a new constructor called InputFeatures\n",
        "\n",
        "Pretty straight forward, even if we are diving a little deep. Lets look at InputFeatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97eGN_JKGYgz",
        "colab_type": "text"
      },
      "source": [
        "**Sabrina: Note that type_ids has been renamed to segment_ids (I think)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vJalJJ7d1HcW"
      },
      "source": [
        "######################### SOURCE CODE ###############################\n",
        "```\n",
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               label_id,\n",
        "               is_real_example=True):\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.label_id = label_id\n",
        "    self.is_real_example = is_real_example\n",
        "```\n",
        "######################### SOURCE CODE ###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F-AxdIXs1HcX"
      },
      "source": [
        "Aaaaaaand InputFeatures is just a class to help hold all of the info.... so we're done!\n",
        "\n",
        "Note these positional embeddings that BERT talks about dont enter into our inputs, those and the summing of the vectors is part of the inner workings of BERT that we havn't got to yet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9lT4sPAE1HcZ",
        "outputId": "0a0fc42b-aee3-4e75-8f88-b1fcef1d9371",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(bert_module.get_input_info_dict(signature='tokens'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'segment_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'input_mask': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>, 'input_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e3sOkuVa1Hcb",
        "outputId": "ef35bcfc-537b-464d-ffef-f3894e8b6f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "print(\"Length of training inputs matches with the original size of our dataset nicely\", len(train_features))\n",
        "print(\"And we can see the components that make up each training example, for example the vocab_ids of example 1:\\n\", train_features[0].input_ids)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training inputs matches with the original size of our dataset nicely 12000\n",
            "And we can see the components that make up each training example, for example the vocab_ids of example 1:\n",
            " [101, 3892, 2183, 2000, 2156, 29379, 7299, 1999, 4164, 1009, 5958, 6358, 5420, 18178, 2015, 5420, 1024, 1007, 1007, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2wC4EWcJ1Hcd"
      },
      "source": [
        "Note the 'train_features' are still python objects (lists etc) and BERT wants tensors, however how we parse in tensors is actually dealt with as part of the model in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ccp5trMwRtmr"
      },
      "source": [
        "# 3.0 Creating a model\n",
        "\n",
        "Now we have our data in a form BERT can hopefully understand (it's still lacking the conversion to tesnors) we can build out a bigger model. \n",
        "\n",
        "But first a bit of terminology. Whoever attended mine and Steve's talk on transfer learning heard us talk about a 3 stage approach: train, fine-tune, classification. This was based on ULMFiT, here a similar aproach is used but without doing stage 2... and confusingly refering to our stage 3 as 'fine-tuning' so just watch out when you hear these terms. It is important to understand exactly what we're doing. Here as we're looking at BERT we'll go with their terminaology, training TL classifiers is 'fine-tuning'.\n",
        "\n",
        "Now the workflow standard TF is to use estimators, that is the example given officially and they do make things \"easier\" once you understand whats going on... the hard part is understanding whats going on. Estimators are high-level apis for dealing with the loading of data into a model and the training of the model. I shall give a quick overview here highlighting where BERT black box code comes in and how it links to the general case. For a more complete overview please look here https://www.youtube.com/watch?v=BhQW2OLzx_c and on the official TF site https://www.tensorflow.org/guide/estimators.\n",
        "\n",
        "In general the work flow is shown in this chart.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "To run through quickly we want to call the train method (actually there are other methods like predict etc we will want to call later, but they all work in the same way so for here ill stick with training) on an estimator. This needs a train_input_fn which deals with data loading. We also have to initilise the estimator object which involves creating a config instace, and a model_fn, the model_fn is made by a model_fn creator which calls a create_model function...\n",
        "\n",
        "So to simplify this down and understand this we really need to build 3 things:\n",
        "- An instance of tf.estimator.RunConfig\n",
        "- A train_input_fn which is a specific kind of function needed to run the estimator to train the data\n",
        "- A model_fn which the estimator wraps around and \"talks to\" the train_input_function\n",
        "\n",
        "There are couple of \"function builders\" but they are just wrappers that we dont absolutely need..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qVL_aeLS1Hce"
      },
      "source": [
        "# 3.1 tf.estimator.RunConfig\n",
        "\n",
        "We start with this part not becasue its the most logical to start with, but because its the easiest! We can get this out of the way quickly, as shown below all we need to do is define a config class on how and where we save the model as it trains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fw1JZjsD1Hcf",
        "colab": {}
      },
      "source": [
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 500\n",
        "SAVE_SUMMARY_STEPS = 100\n",
        "\n",
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(\n",
        "    model_dir=\"/content/gdrive/My Drive/DeepLearning/english-large/models\",\n",
        "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wTbbm4SP1Hch"
      },
      "source": [
        "# 3.2 train_input_fn\n",
        "\n",
        "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. The estimator we will build needs this, according to the official estimators docmentation this function needs to...\n",
        "\n",
        "\"A function that provides input data for training as minibatches. See Premade Estimators for more information. The function should construct and return one of the following: * A tf.data.Dataset object: Outputs of Dataset object must be a tuple (features, labels) with same constraints as below. * A tuple (features, labels): Where features is a tf.Tensor or a dictionary of string feature name to Tensor and labels is a Tensor or a dictionary of string label name to Tensor. Both features and labels are consumed by model_fn. They should satisfy the expectation of model_fn from inputs.\"\n",
        "\n",
        "Now it appears BERT has another black box creator for us, but we can pull that apart and see whats happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Pv2bAlOX_-K",
        "colab": {}
      },
      "source": [
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = bert.run_classifier.input_fn_builder(\n",
        "    features=train_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r-46yAc71Hcl"
      },
      "source": [
        "Now looking at the BERT code bert.run_classifier.input_fn_builder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zNV3rZo61Hcl"
      },
      "source": [
        "######################### SOURCE CODE ###############################\n",
        "```\n",
        "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn\n",
        "```\n",
        "######################### SOURCE CODE ###############################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7yhi4n1o1Hcm"
      },
      "source": [
        "We can see that the BERT function builder is doing what estimators expects, its building and returning a shuffled, repeated, batch of data (these things could be tweaked depending on the project if we went in manually) of batch_size. With all of the components that we need in the form of tf.data.Dataset. We can also see these 4 components match exactly the attribute names of the objects returned in train_features as given by the InputFeatures class.\n",
        "\n",
        "We also know this should talk to our model function (even though we've not made it yet) as we know from the TF Hub that we expect 3 of these 4 inputs in the form of tensors, the label_ids is the extra bit for our classification task.\n",
        "\n",
        "We can see how the train_input_fn works if we pass some dummy params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rDT-FCpr1Hcm",
        "outputId": "d541c38c-2d4c-4490-eb84-ef48396afe11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_input_fn({'batch_size': 32})"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: {input_ids: (?, 128), input_mask: (?, 128), segment_ids: (?, 128), label_ids: (?,)}, types: {input_ids: tf.int32, input_mask: tf.int32, segment_ids: tf.int32, label_ids: tf.int32}>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ts5tjCQ71Hcp",
        "outputId": "df8a821e-0178-4b65-f982-132541823773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "bert_module.get_input_info_dict(signature='tokens')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>,\n",
              " 'input_mask': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>,\n",
              " 'segment_ids': <hub.ParsedTensorInfo shape=(?, ?) dtype=int32 is_sparse=False>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WhwCmBjS1Hcs"
      },
      "source": [
        "# 3.3 model_fn\n",
        "\n",
        "Finally we arrive at the juciy bit, the model! this is the bit where we really have to disect the BERT black box code to understand fully whats going on and use this in the most agnostic manner. Now we already know we need to use TF Hub to get the model and weights, we also know we need it to take in at least 3 inputs to satisfy the model (we are actually passing 4 because we need our label_id). However ho the training is managed we still need to investigate.\n",
        "\n",
        "Below is the model from the official tutorial, but I have added some annotations.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6o2a5ZIvRcJq",
        "colab": {}
      },
      "source": [
        "def create_model(is_predicting, #Bool to say if we're predicting (or training) as the output we'd like from the model would move\n",
        "                 input_ids, # We know that our train_input_fn will pass the inputs as tensor of mini_batch_size, they\n",
        "                 #actually come as one dictionary but in the next cell we see that in model_fn we split them out\n",
        "                 input_mask, \n",
        "                 segment_ids, \n",
        "                 labels,\n",
        "                 num_labels): # Num labels just lets us define the size of our softmax level for classification\n",
        "    \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "    # Get the BERT model again from tf hub, this time setting trainable as true as we will fine tune it\n",
        "    bert_module = hub.Module(\n",
        "      BERT_MODEL_HUB,\n",
        "      trainable=True)\n",
        "    \n",
        "    # We have our input tensors, the ones for the pre-trained bert are just being grouped here for convinience\n",
        "    bert_inputs = dict(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids)\n",
        "    \n",
        "    # We already saw earlier in the notebook, 'tokens' is the standard endpoint that gives the reference tensors coming out of BERT\n",
        "    # Unlike 'tokenization_info' here we need to pass the inputs to get the outputs.\n",
        "    bert_outputs = bert_module(\n",
        "      inputs=bert_inputs,\n",
        "      signature=\"tokens\",\n",
        "      as_dict=True)\n",
        "\n",
        "    # From the tf hub docs for BERT we know we can use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "    # Or use \"sequence_outputs\" for token-level output.\n",
        "    output_layer = bert_outputs[\"pooled_output\"]\n",
        "\n",
        "    hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  # Create our own classification layer\n",
        "    output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "    output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "    with tf.variable_scope(\"loss\"):\n",
        "\n",
        "        # Dropout helps prevent overfitting\n",
        "        output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "        logits = tf.nn.bias_add(logits, output_bias)\n",
        "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "        # Convert labels into one-hot encoding\n",
        "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
        "        # If we're predicting, we want predicted labels and the probabiltiies.\n",
        "        if is_predicting:\n",
        "              return (predicted_labels, log_probs)\n",
        "\n",
        "        # If we're train/eval, compute loss between predicted and actual label\n",
        "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "        loss = tf.reduce_mean(per_example_loss)\n",
        "        return (loss, predicted_labels, log_probs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qpE0ZIDOCQzE"
      },
      "source": [
        "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction. Remeber its the actual 'model_fn' we want, the rest is extra abstraction commonly used to make things \"easier\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FnH-AnOQ9KKW",
        "colab": {}
      },
      "source": [
        "# model_fn_builder actually creates our model function\n",
        "# using the passed parameters for num_labels, learning_rate, etc.\n",
        "def model_fn_builder(num_labels, \n",
        "                     learning_rate, \n",
        "                     num_train_steps,\n",
        "                     num_warmup_steps):\n",
        "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "    def model_fn(features, \n",
        "                 labels, \n",
        "                 mode, \n",
        "                 params): \n",
        "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "        input_ids = features[\"input_ids\"]\n",
        "        input_mask = features[\"input_mask\"]\n",
        "        segment_ids = features[\"segment_ids\"]\n",
        "        label_ids = features[\"label_ids\"]\n",
        "\n",
        "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
        "\n",
        "        # TRAIN and EVAL\n",
        "        if not is_predicting:\n",
        "\n",
        "            # Here we run our model and return our loss function and friends\n",
        "            (loss, predicted_labels, log_probs) = create_model(\n",
        "            is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "            # Here we define a training op, this one comes from BERT we will open this up in a second and have a look whats going on\n",
        "            # but logically it makes sense what its doing, actually running the training\n",
        "            train_op = bert.optimization.create_optimizer(\n",
        "              loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "            # Calculate evaluation metrics and call them. This is a little subtle, the types of metrics we can use are limited to\n",
        "            # tf's tf.metrics library which is definetly not exaustive (no multiclass f1 score...), as such what we can actually \n",
        "            # do is largely ignore this and calculate the metrics in python with the returned probabilites on the test set\n",
        "            def metric_fn(label_ids, predicted_labels):\n",
        "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
        "                return {\"eval_accuracy\": accuracy}\n",
        "\n",
        "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
        "\n",
        "            # Next we create a different end point for model_fn depending on what we're doing (training, predicting, etc)\n",
        "            # All of these are instances of tf.estimator.EstimatorSpec which we will unpack below\n",
        "\n",
        "            if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "                return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                    loss=loss,\n",
        "                    train_op=train_op)\n",
        "            else:\n",
        "                return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                    loss=loss,\n",
        "                    eval_metric_ops=eval_metrics)\n",
        "        else:\n",
        "            (predicted_labels, log_probs) = create_model(\n",
        "               is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
        "\n",
        "            predictions = {\n",
        "              'probabilities': log_probs,\n",
        "              'labels': predicted_labels\n",
        "            }\n",
        "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
        "\n",
        "    # Return the actual model function in the closure\n",
        "    return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "19zOtAYI1Hc0"
      },
      "source": [
        "So the model_fn_builder and the model_fn are pretty clear with a couple of key things we'd like to deep-dive. \n",
        "\n",
        "- whats making our train_op inside bert.optimization.create_optimizer\n",
        "- whats this: tf.estimator.EstimatorSpec? And how do we make one for an arbitrary model_fn going into an estimator\n",
        "\n",
        "First lets look at bert.optimization.create_optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w6A8IQke1Hc3"
      },
      "source": [
        "## bert.optimization.create_optimizer\n",
        "\n",
        "######################### SOURCE CODE ###############################\n",
        "```\n",
        "def create_optimizer(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu):\n",
        "  \"\"\"Creates an optimizer training op.\"\"\"\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "\n",
        "  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)\n",
        "\n",
        "  # Implements linear decay of the learning rate.\n",
        "  learning_rate = tf.train.polynomial_decay(\n",
        "      learning_rate,\n",
        "      global_step,\n",
        "      num_train_steps,\n",
        "      end_learning_rate=0.0,\n",
        "      power=1.0,\n",
        "      cycle=False)\n",
        "\n",
        "  # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
        "  # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
        "  if num_warmup_steps:\n",
        "    global_steps_int = tf.cast(global_step, tf.int32)\n",
        "    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)\n",
        "\n",
        "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
        "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
        "\n",
        "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
        "    warmup_learning_rate = init_lr * warmup_percent_done\n",
        "\n",
        "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
        "    learning_rate = (\n",
        "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
        "\n",
        "  # It is recommended that you use this optimizer for fine tuning, since this\n",
        "  # is how the model was trained (note that the Adam m/v variables are NOT\n",
        "  # loaded from init_checkpoint.)\n",
        "  optimizer = AdamWeightDecayOptimizer(\n",
        "      learning_rate=learning_rate,\n",
        "      weight_decay_rate=0.01,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999,\n",
        "      epsilon=1e-6,\n",
        "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
        "```\n",
        "######################### SOURCE CODE ###############################\n",
        "\n",
        "This is all quite straightforward, they are just defining an Adam optimizer with a linear learning rate decay. In addition they have the option of \"warmup\" which is the same logic FastAI used with ULMFiT in have slantend triangular learning rates, the idea is that you start low, build up to your max learning rate, and then linearly decrease.\n",
        "\n",
        "## tf.estimator.EstimatorSpec\n",
        "\n",
        "This is a standard class and is quite straightforward. As per the docs https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec the EstimatorSpec is what is to be returned from any model  function and has various attributes around the mode, loss, train_op, etc and these must \"fully define\" the model to be run by the estimator. So the model function is just defineing the model and filling in this class object with the relevant tf.variables to be ran."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eCJ9so_U1Hc4"
      },
      "source": [
        "Now we have all of the (confusing) constituent pieces, we can put them together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjwJ4bTeWXD8",
        "outputId": "ecdc2f2e-ff78-4eb3-a5f4-310a1f11a76a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Compute train and warmup steps from batch size\n",
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "# Warmup is a period of time where hte learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "\n",
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "print(num_train_steps)\n",
        "print(num_warmup_steps)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1125\n",
            "112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q_WebpS1X97v",
        "outputId": "e0871522-1d02-4194-bcbe-1f3b5c41088b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "    num_labels=len(label_list),\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    params={\"batch_size\": BATCH_SIZE})"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/content/gdrive/My Drive/DeepLearning/english-large/models', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f06072ad668>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': '/content/gdrive/My Drive/DeepLearning/english-large/models', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f06072ad668>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4KCRIgjx1Hc-"
      },
      "source": [
        "# 4.0 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t6Nukby2EB6-"
      },
      "source": [
        "Now we train our model! We can use a Colab notebook running on Google's GPUs, or just do it locally.\n",
        "\n",
        "Remember we built the estimator around the model_fn, now to call it we use the train_input_fn, we can also create other input functions and use them with the same estimator, depending on what data we want to pass (is it trian, test, val, adhoc predictions). All input functions look rather the same, as the diferentiation between training, evaluating, and predicting is determined by what method of the estimator we call, this sets a key (e.g. tf.estimator.ModeKeys.PREDICT) that our model_fn can deal with.\n",
        "\n",
        "WARNING: BERTs pretty fat (as previously discussed) if you get a OOM error, or Dst tensor is not initialized, or something weird it could well be your out of memory (VRAM), I tried running this on my work and home laptop (home laptop having 4GB VRAM) and still got OOM, if thats the case I reccomend running it in colab, im not sure how much you are alloted, I feel it varies depending on demand... but every time I have tried I was able to run this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nucD4gluYJmK",
        "outputId": "871441a8-171e-46f0-9ca1-47c743307986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(f'Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-23-5751825f1cbf>:45: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-23-5751825f1cbf>:45: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.0896733, step = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 1.0896733, step = 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.562053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.562053\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6942352, step = 100 (177.923 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.6942352, step = 100 (177.923 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632609\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.7475168, step = 200 (158.074 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.7475168, step = 200 (158.074 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632714\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.8537044, step = 300 (158.050 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.8537044, step = 300 (158.050 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632463\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632463\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.66730016, step = 400 (158.111 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.66730016, step = 400 (158.111 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 500 into /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 500 into /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.602391\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.602391\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.35528386, step = 500 (166.005 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.35528386, step = 500 (166.005 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632509\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632509\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.20447728, step = 600 (158.106 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.20447728, step = 600 (158.106 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632741\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.12339158, step = 700 (158.042 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.12339158, step = 700 (158.042 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632449\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632449\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.18937725, step = 800 (158.112 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.18937725, step = 800 (158.112 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632411\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.21705556, step = 900 (158.127 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.21705556, step = 900 (158.127 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 1000 into /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 1000 into /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.607915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.607915\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.24174243, step = 1000 (164.493 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.24174243, step = 1000 (164.493 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 0.632029\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.10107264, step = 1100 (158.224 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 0.10107264, step = 1100 (158.224 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 1125 into /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 1125 into /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.12683146.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Loss for final step: 0.12683146.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training took time  0:31:19.463027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CmbLTVniARy3"
      },
      "source": [
        "Now let's use our test data to see how well our model did. Notice on the estimator call we call evaluate so our model function knows what to do (evaluate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JIhejfpyJ8Bx",
        "colab": {}
      },
      "source": [
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PPVEXhNjYXC-",
        "outputId": "25971eb3-77ab-4539-c5a5-43c9d1131842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-11-22T13:26:15Z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-11-22T13:26:15Z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-1125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-1125\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-11-22-13:27:17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-11-22-13:27:17\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 1125: eval_accuracy = 0.73966664, global_step = 1125, loss = 0.82600015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 1125: eval_accuracy = 0.73966664, global_step = 1125, loss = 0.82600015\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1125: /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-1125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1125: /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-1125\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.73966664, 'global_step': 1125, 'loss': 0.82600015}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W5MibvY11HdW"
      },
      "source": [
        "This called the evaluate method which can return a series of metrics automatically. However as mentioned there isnt the best choice of metrics available natively in Tensorflow so we will actually demo making predictions by making predictions on the whole test set and then evaluating our own metrics in Python.\n",
        "\n",
        "Also note: BERTs pretty good :o\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OokM9V-pcfYn",
        "colab_type": "text"
      },
      "source": [
        "#### export model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgmKSQ_tXRRF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "03b894c6-ce9e-4f30-fbd4-0baa7c2bc6f1"
      },
      "source": [
        "# evaluating on pre-trained model\n",
        "# model.ckpt-500.data-00000-of-00001\n",
        "# according to https://github.com/tensorflow/models/issues/2676#issuecomment-444242182, you need to drop everything after ckpt in the filename\n",
        "checkpoint_path = \"/content/gdrive/My Drive/DeepLearning/english-large/models_trained_on_reviews/model.ckpt-500\"\n",
        "estimator.evaluate(input_fn=test_input_fn, checkpoint_path=checkpoint_path, steps=None)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-11-20T15:23:54Z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-11-20T15:23:54Z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models_trained_on_reviews/model.ckpt-500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models_trained_on_reviews/model.ckpt-500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-11-20-15:25:12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-11-20-15:25:12\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 500: eval_accuracy = 0.5736667, global_step = 500, loss = 0.8834836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 500: eval_accuracy = 0.5736667, global_step = 500, loss = 0.8834836\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: /content/gdrive/My Drive/DeepLearning/english-large/models_trained_on_reviews/model.ckpt-500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: /content/gdrive/My Drive/DeepLearning/english-large/models_trained_on_reviews/model.ckpt-500\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.5736667, 'global_step': 500, 'loss': 0.8834836}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0q6dEOL3yRq",
        "colab_type": "text"
      },
      "source": [
        "## Save model and try to use it again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMrgXezm34zh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "3607fcd5-8235-4c61-edc3-2fa3405e8266"
      },
      "source": [
        "# I found this solution here: https://github.com/bigboNed3/bert_serving\n",
        "# which I found https://github.com/google-research/bert/issues/146#issuecomment-446622209\n",
        "\n",
        "def serving_input_fn():\n",
        "    label_ids = tf.placeholder(tf.int32, [None], name='label_ids')\n",
        "    input_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_ids')\n",
        "    input_mask = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='input_mask')\n",
        "    segment_ids = tf.placeholder(tf.int32, [None, MAX_SEQ_LENGTH], name='segment_ids')\n",
        "    input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({\n",
        "        'label_ids': label_ids,\n",
        "        'input_ids': input_ids,\n",
        "        'input_mask': input_mask,\n",
        "        'segment_ids': segment_ids,\n",
        "    })()\n",
        "    return input_fn\n",
        "\n",
        "\n",
        "exported_path = estimator.export_saved_model(\"/content/gdrive/My Drive/DeepLearning/english-large/models/tweet_model/\", serving_input_receiver_fn=serving_input_fn)\n",
        "print(exported_path)\n",
        "\n",
        "# '/content/gdrive/My Drive/DeepLearning/english-large/models/tweet_model/1574438139'\n",
        "# /saved_model.pb"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-1125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-1125\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets added to graph.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/DeepLearning/english-large/models/tweet_model/1/temp-b'1574438564'/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/DeepLearning/english-large/models/tweet_model/1/temp-b'1574438564'/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /content/gdrive/My Drive/DeepLearning/english-large/models/tweet_model/1/temp-b'1574438564'/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:SavedModel written to: /content/gdrive/My Drive/DeepLearning/english-large/models/tweet_model/1/temp-b'1574438564'/saved_model.pb\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "b'/content/gdrive/My Drive/DeepLearning/english-large/models/tweet_model/1/1574438564'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDk36AM_clGH",
        "colab_type": "text"
      },
      "source": [
        "#### Read saved model from file\n",
        "\n",
        "Note that this might be rather slow for on-demand classification. for this we might want to look into techniques described here: https://guillaumegenthial.github.io/serving-tensorflow-estimator.html#exporting-the-estimator-as-a-tfsaved_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrXytGpwcqt1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "72b3858e-5a53-43c5-8f48-d7173a65a2b2"
      },
      "source": [
        "export_path = '/content/gdrive/My Drive/DeepLearning/english-large/models/tweet_model/1574438139'\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  imported = tf.saved_model.load(sess, None, export_path)\n",
        "\n",
        "# with tf.Graph().as_default():\n",
        "#     bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "#     tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "#     with tf.Session() as sess:\n",
        "#         vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "#                                               tokenization_info[\"do_lower_case\"]])\n",
        "\n",
        "\n",
        "\n",
        "# sess, tags, export_dir,\n",
        "\n",
        "# sess: The TensorFlow session to restore the variables.\n",
        "# tags: Set of string tags to identify the required MetaGraphDef. These should correspond to the tags used when saving the variables using the SavedModel save() API.\n",
        "# export_dir: Directory in which the SavedModel protocol buffer and variables to be loaded are located.\n",
        "\n",
        "\n",
        "# tf.compat.v1.enable_resource_variables()\n",
        "# tf.enable_resource_variables()\n",
        "# imported = tf.compat.v1.saved_model.load_v2(export_path)\n",
        "print(type(imported))\n",
        "\n",
        "# estimator2 = tf.estimator.Estimator(\n",
        "#     model_fn=model_fn,\n",
        "#     config=run_config,\n",
        "#     params={\"batch_size\": BATCH_SIZE})"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-ae2457e9490f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mimported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# with tf.Graph().as_default():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(sess, tags, export_dir, import_scope, **saver_kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m   \"\"\"\n\u001b[1;32m    268\u001b[0m   \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSavedModelLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msaver_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, sess, tags, import_scope, **saver_kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       saver, _ = self.load_graph(sess.graph, tags, import_scope,\n\u001b[0;32m--> 422\u001b[0;31m                                  **saver_kwargs)\n\u001b[0m\u001b[1;32m    423\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_init_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mload_graph\u001b[0;34m(self, graph, tags, import_scope, **saver_kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m           \u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmay\u001b[0m \u001b[0mbe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \"\"\"\n\u001b[0;32m--> 349\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_meta_graph_def_from_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m       return tf_saver._import_meta_graph_with_return_elements(  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mget_meta_graph_def_from_tags\u001b[0;34m(self, tags)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmeta_graph_def\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mavailable_tags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_info_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_info_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mmeta_graph_def_to_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mfound_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPXkOwvR1xJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "593d3068-0bb0-459f-cde0-0eba03d293f1"
      },
      "source": [
        "imported"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.tracking.AutoTrackable at 0x7fecc7aa3cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQujoInoFs3w",
        "colab_type": "text"
      },
      "source": [
        "### ADD-1 evaluate on a different test set (this is for when you want to go through different test sets and compare)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VTAiFfnFriw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "86e56f43-7bc1-455d-fcda-5e7e7863850d"
      },
      "source": [
        "# creating a new test set (in the format that Bert expects)\n",
        "\n",
        "df_tst = pd.read_csv(\"/content/gdrive/My Drive/DeepLearning/english-large/tweet_test.csv\")\n",
        "df_tst = df_tst[[\"text\", \"label\"]]\n",
        "\n",
        "# df_tst['label'] = df_tst['label'].apply(translate)\n",
        "df_tst['text'] = df_tst['text'].astype('str')\n",
        "g = df_tst.groupby('label')\n",
        "df_tst = g.apply(lambda x: x.sample(n=g.size().min(), random_state=1).reset_index(drop=True))\n",
        "\n",
        "df_tst = df_tst.sample(frac=1)\n",
        "\n",
        "test_InputExamples = df_tst.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)\n",
        "\n",
        "estimator.evaluate(input_fn=test_input_fn, steps=None)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 3000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] anyone want to see paper towns on thursday ? ? [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] anyone want to see paper towns on thursday ? ? [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3087 2215 2000 2156 3259 4865 2006 9432 1029 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3087 2215 2000 2156 3259 4865 2006 9432 1029 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] someone sat down and wrote this chorus . i ' m fed up . \" we are the new americana high on legal marijuana raised on big ##gie and nirvana \" [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] someone sat down and wrote this chorus . i ' m fed up . \" we are the new americana high on legal marijuana raised on big ##gie and nirvana \" [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2619 2938 2091 1998 2626 2023 7165 1012 1045 1005 1049 7349 2039 1012 1000 2057 2024 1996 2047 25988 2152 2006 3423 16204 2992 2006 2502 11239 1998 26530 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2619 2938 2091 1998 2626 2023 7165 1012 1045 1005 1049 7349 2039 1012 1000 2057 2024 1996 2047 25988 2152 2006 3423 16204 2992 2006 2502 11239 1998 26530 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] right ? even if he wasn ' t the target , he ' s on the block & amp ; it ' s big brother , c ' mon . # bb ##17 https : / / t . co / mn ##i ##2 ##v ##6 ##k ##1 ##pm [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] right ? even if he wasn ' t the target , he ' s on the block & amp ; it ' s big brother , c ' mon . # bb ##17 https : / / t . co / mn ##i ##2 ##v ##6 ##k ##1 ##pm [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2157 1029 2130 2065 2002 2347 1005 1056 1996 4539 1010 2002 1005 1055 2006 1996 3796 1004 23713 1025 2009 1005 1055 2502 2567 1010 1039 1005 12256 1012 1001 22861 16576 16770 1024 1013 1013 1056 1012 2522 1013 24098 2072 2475 2615 2575 2243 2487 9737 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2157 1029 2130 2065 2002 2347 1005 1056 1996 4539 1010 2002 1005 1055 2006 1996 3796 1004 23713 1025 2009 1005 1055 2502 2567 1010 1039 1005 12256 1012 1001 22861 16576 16770 1024 1013 1013 1056 1012 2522 1013 24098 2072 2475 2615 2575 2243 2487 9737 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] super eagles coach sunday ol ##ise ##h has revealed chelsea midfielder john mike ##l s ##nu ##bbed his calls in london while he attempted to speak with him [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] super eagles coach sunday ol ##ise ##h has revealed chelsea midfielder john mike ##l s ##nu ##bbed his calls in london while he attempted to speak with him [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3565 8125 2873 4465 19330 5562 2232 2038 3936 9295 8850 2198 3505 2140 1055 11231 15499 2010 4455 1999 2414 2096 2002 4692 2000 3713 2007 2032 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3565 8125 2873 4465 19330 5562 2232 2038 3936 9295 8850 2198 3505 2140 1055 11231 15499 2010 4455 1999 2414 2096 2002 4692 2000 3713 2007 2032 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] janet jackson is coming on september 18th i will give someone $ 50 to shoot her in the fucking face [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] janet jackson is coming on september 18th i will give someone $ 50 to shoot her in the fucking face [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9965 4027 2003 2746 2006 2244 4985 1045 2097 2507 2619 1002 2753 2000 5607 2014 1999 1996 8239 2227 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9965 4027 2003 2746 2006 2244 4985 1045 2097 2507 2619 1002 2753 2000 5607 2014 1999 1996 8239 2227 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-11-15T17:50:51Z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-11-15T17:50:51Z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-11-15-17:51:46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-11-15-17:51:46\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 5593: eval_accuracy = 0.5223333, global_step = 5593, loss = 1.2569385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 5593: eval_accuracy = 0.5223333, global_step = 5593, loss = 1.2569385\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5593: /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5593: /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.5223333, 'global_step': 5593, 'loss': 1.2569385}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "86e56f43-7bc1-455d-fcda-5e7e7863850d",
        "id": "xROE4fe-SDJB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# creating a new test set (in the format that Bert expects), evaluating it from previously saved checkpoints\n",
        "\n",
        "df_tst = pd.read_csv(\"/content/gdrive/My Drive/DeepLearning/english-large/tweet_test.csv\")\n",
        "df_tst = df_tst[[\"text\", \"label\"]]\n",
        "\n",
        "# df_tst['label'] = df_tst['label'].apply(translate)\n",
        "df_tst['text'] = df_tst['text'].astype('str')\n",
        "g = df_tst.groupby('label')\n",
        "df_tst = g.apply(lambda x: x.sample(n=g.size().min(), random_state=1).reset_index(drop=True))\n",
        "\n",
        "df_tst = df_tst.sample(frac=1)\n",
        "\n",
        "test_InputExamples = df_tst.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
        "                                                                   text_a = x[DATA_COLUMN], \n",
        "                                                                   text_b = None, \n",
        "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
        "\n",
        "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "\n",
        "test_input_fn = run_classifier.input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)\n",
        "\n",
        "checkpoint_path = \"DeepLearning/english-large/models_trained_on_reviews\"\n",
        "estimator.evaluate(input_fn=test_input_fn, checkpoint_path=checkpoint_path, steps=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 3000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] anyone want to see paper towns on thursday ? ? [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] anyone want to see paper towns on thursday ? ? [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3087 2215 2000 2156 3259 4865 2006 9432 1029 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3087 2215 2000 2156 3259 4865 2006 9432 1029 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] someone sat down and wrote this chorus . i ' m fed up . \" we are the new americana high on legal marijuana raised on big ##gie and nirvana \" [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] someone sat down and wrote this chorus . i ' m fed up . \" we are the new americana high on legal marijuana raised on big ##gie and nirvana \" [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2619 2938 2091 1998 2626 2023 7165 1012 1045 1005 1049 7349 2039 1012 1000 2057 2024 1996 2047 25988 2152 2006 3423 16204 2992 2006 2502 11239 1998 26530 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2619 2938 2091 1998 2626 2023 7165 1012 1045 1005 1049 7349 2039 1012 1000 2057 2024 1996 2047 25988 2152 2006 3423 16204 2992 2006 2502 11239 1998 26530 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] right ? even if he wasn ' t the target , he ' s on the block & amp ; it ' s big brother , c ' mon . # bb ##17 https : / / t . co / mn ##i ##2 ##v ##6 ##k ##1 ##pm [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] right ? even if he wasn ' t the target , he ' s on the block & amp ; it ' s big brother , c ' mon . # bb ##17 https : / / t . co / mn ##i ##2 ##v ##6 ##k ##1 ##pm [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2157 1029 2130 2065 2002 2347 1005 1056 1996 4539 1010 2002 1005 1055 2006 1996 3796 1004 23713 1025 2009 1005 1055 2502 2567 1010 1039 1005 12256 1012 1001 22861 16576 16770 1024 1013 1013 1056 1012 2522 1013 24098 2072 2475 2615 2575 2243 2487 9737 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2157 1029 2130 2065 2002 2347 1005 1056 1996 4539 1010 2002 1005 1055 2006 1996 3796 1004 23713 1025 2009 1005 1055 2502 2567 1010 1039 1005 12256 1012 1001 22861 16576 16770 1024 1013 1013 1056 1012 2522 1013 24098 2072 2475 2615 2575 2243 2487 9737 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] super eagles coach sunday ol ##ise ##h has revealed chelsea midfielder john mike ##l s ##nu ##bbed his calls in london while he attempted to speak with him [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] super eagles coach sunday ol ##ise ##h has revealed chelsea midfielder john mike ##l s ##nu ##bbed his calls in london while he attempted to speak with him [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3565 8125 2873 4465 19330 5562 2232 2038 3936 9295 8850 2198 3505 2140 1055 11231 15499 2010 4455 1999 2414 2096 2002 4692 2000 3713 2007 2032 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3565 8125 2873 4465 19330 5562 2232 2038 3936 9295 8850 2198 3505 2140 1055 11231 15499 2010 4455 1999 2414 2096 2002 4692 2000 3713 2007 2032 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 3 (id = 1)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: None\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] janet jackson is coming on september 18th i will give someone $ 50 to shoot her in the fucking face [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] janet jackson is coming on september 18th i will give someone $ 50 to shoot her in the fucking face [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9965 4027 2003 2746 2006 2244 4985 1045 2097 2507 2619 1002 2753 2000 5607 2014 1999 1996 8239 2227 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9965 4027 2003 2746 2006 2244 4985 1045 2097 2507 2619 1002 2753 2000 5607 2014 1999 1996 8239 2227 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-11-15T17:50:51Z\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting evaluation at 2019-11-15T17:50:51Z\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-11-15-17:51:46\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished evaluation at 2019-11-15-17:51:46\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 5593: eval_accuracy = 0.5223333, global_step = 5593, loss = 1.2569385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving dict for global step 5593: eval_accuracy = 0.5223333, global_step = 5593, loss = 1.2569385\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5593: /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5593: /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_accuracy': 0.5223333, 'global_step': 5593, 'loss': 1.2569385}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-eamcKyHqmK",
        "colab_type": "text"
      },
      "source": [
        "## Predict\n",
        "To make predictions we need a quick pipeline that does all of the data-prep stages we mentioned in this notebook, we can actually do that super cleanly when we dont spend ages pulling everything apart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OsrbTD2EJTVl",
        "colab": {}
      },
      "source": [
        "def get_prediction(in_sentences):\n",
        "    \"\"\"\n",
        "    Helper function to run the pipeline for a list of examples to classify\n",
        "    \"\"\"\n",
        "    labels = [1,3,5]\n",
        "    \n",
        "    # As in 2.1. get input exaple objects in a list for every prediction example\n",
        "    input_examples = [bert.run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 1) for x in in_sentences]\n",
        "\n",
        "    # As in 2.3 convert to the classes that BERT will ultimately understand (still in python format here)\n",
        "    input_features = bert.run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "    \n",
        "    # As in 3.2 we need an input function for the estimator, this just converts the data above to tensors to serve to the estimator\n",
        "    predict_input_fn = bert.run_classifier.input_fn_builder(features=input_features, \n",
        "                                                            seq_length=MAX_SEQ_LENGTH, \n",
        "                                                            is_training=False, \n",
        "                                                            drop_remainder=False)\n",
        "    \n",
        "    # Run the model this time on .predict which as defined by our model_fn will return probabilities and labels\n",
        "    predictions = estimator.predict(predict_input_fn)\n",
        "    \n",
        "    # Returns a list of tuples (sentence, probs, label) for each example we passed\n",
        "    return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GdJJ1ekG1HdZ",
        "outputId": "74179828-7f4a-449d-8402-29d39946ae26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "predictions = get_prediction(df_tst[DATA_COLUMN])\n",
        "pred_labels = [pred[2] for pred in predictions]"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 3000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] anyone want to see paper towns on thursday ? ? [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] anyone want to see paper towns on thursday ? ? [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3087 2215 2000 2156 3259 4865 2006 9432 1029 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3087 2215 2000 2156 3259 4865 2006 9432 1029 1029 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] someone sat down and wrote this chorus . i ' m fed up . \" we are the new americana high on legal marijuana raised on big ##gie and nirvana \" [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] someone sat down and wrote this chorus . i ' m fed up . \" we are the new americana high on legal marijuana raised on big ##gie and nirvana \" [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2619 2938 2091 1998 2626 2023 7165 1012 1045 1005 1049 7349 2039 1012 1000 2057 2024 1996 2047 25988 2152 2006 3423 16204 2992 2006 2502 11239 1998 26530 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2619 2938 2091 1998 2626 2023 7165 1012 1045 1005 1049 7349 2039 1012 1000 2057 2024 1996 2047 25988 2152 2006 3423 16204 2992 2006 2502 11239 1998 26530 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] right ? even if he wasn ' t the target , he ' s on the block & amp ; it ' s big brother , c ' mon . # bb ##17 https : / / t . co / mn ##i ##2 ##v ##6 ##k ##1 ##pm [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] right ? even if he wasn ' t the target , he ' s on the block & amp ; it ' s big brother , c ' mon . # bb ##17 https : / / t . co / mn ##i ##2 ##v ##6 ##k ##1 ##pm [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2157 1029 2130 2065 2002 2347 1005 1056 1996 4539 1010 2002 1005 1055 2006 1996 3796 1004 23713 1025 2009 1005 1055 2502 2567 1010 1039 1005 12256 1012 1001 22861 16576 16770 1024 1013 1013 1056 1012 2522 1013 24098 2072 2475 2615 2575 2243 2487 9737 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2157 1029 2130 2065 2002 2347 1005 1056 1996 4539 1010 2002 1005 1055 2006 1996 3796 1004 23713 1025 2009 1005 1055 2502 2567 1010 1039 1005 12256 1012 1001 22861 16576 16770 1024 1013 1013 1056 1012 2522 1013 24098 2072 2475 2615 2575 2243 2487 9737 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] super eagles coach sunday ol ##ise ##h has revealed chelsea midfielder john mike ##l s ##nu ##bbed his calls in london while he attempted to speak with him [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] super eagles coach sunday ol ##ise ##h has revealed chelsea midfielder john mike ##l s ##nu ##bbed his calls in london while he attempted to speak with him [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3565 8125 2873 4465 19330 5562 2232 2038 3936 9295 8850 2198 3505 2140 1055 11231 15499 2010 4455 1999 2414 2096 2002 4692 2000 3713 2007 2032 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 3565 8125 2873 4465 19330 5562 2232 2038 3936 9295 8850 2198 3505 2140 1055 11231 15499 2010 4455 1999 2414 2096 2002 4692 2000 3713 2007 2032 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] janet jackson is coming on september 18th i will give someone $ 50 to shoot her in the fucking face [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] janet jackson is coming on september 18th i will give someone $ 50 to shoot her in the fucking face [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9965 4027 2003 2746 2006 2244 4985 1045 2097 2507 2619 1002 2753 2000 5607 2014 1999 1996 8239 2227 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 9965 4027 2003 2746 2006 2244 4985 1045 2097 2507 2619 1002 2753 2000 5607 2014 1999 1996 8239 2227 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5Y6DPEZo1Hdb"
      },
      "source": [
        "Now the below is just an adhoc example of what we can do to evaluate this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BXw2BE5IzzvH",
        "outputId": "86205df4-bbf7-4612-bf4d-095ca91d018f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "\n",
        "    :param cm: Confusion matrix\n",
        "    :param classes: Python list of class names\n",
        "    :param normalize: Boolean to normalize the matrix\n",
        "    :param title: Title of the graph\n",
        "    :param cmap: Colour map\n",
        "    \"\"\"\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, \"{:.2f}\".format(cm[i, j]),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "cm_test = confusion_matrix(y_true=df_tst[\"label\"], y_pred=pred_labels)\n",
        "plot_confusion_matrix(cm_test,\n",
        "                      ['neg', 'neu', 'pos'],\n",
        "                      normalize=False,\n",
        "                      title='Validation')\n",
        "print(\"f1 score\", f1_score(df_tst[\"label\"], pred_labels, average=\"micro\"))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[536 229 235]\n",
            " [241 341 418]\n",
            " [104 203 693]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaMAAAEfCAYAAADsnan6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHIzMzCzrHIzMzCzrHIzMzCzr/j8+xaIMpM7l8AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "f1 score 0.5233333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ueKsULteiz1B"
      },
      "source": [
        "Now we can predict completely made up adhoc examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-thbodgih_VJ",
        "colab": {}
      },
      "source": [
        "pred_sentences = [\n",
        "  \"That movie was absolutely awful\",\n",
        "  \"The acting was a bit lacking\",\n",
        "  \"The film was creative and surprising\",\n",
        "  \"Absolutely fantastic!\"\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QrZmvZySKQTm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5646735c-9402-4a88-8ba5-d6bba6217720"
      },
      "source": [
        "predictions = get_prediction(pred_sentences)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Writing example 0 of 4\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Example ***\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:guid: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] absolutely fantastic ! [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:tokens: [CLS] absolutely fantastic ! [SEP]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:label: 1 (id = 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/DeepLearning/english-large/models/model.ckpt-5593\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MXkRiEBUqN3n"
      },
      "source": [
        "Voila! We have a sentiment classifier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ERkTE8-7oQLZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d5183751-d4ab-4e55-903f-6313e4cb04ab"
      },
      "source": [
        "predictions[0][2]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bjPMaH1FvomF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "025a9ac1-a328-49f1-f979-cbb78ae75aca"
      },
      "source": [
        "predictions\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('That movie was absolutely awful',\n",
              "  array([-2.6890805e-03, -6.3359408e+00, -6.9976740e+00], dtype=float32),\n",
              "  1),\n",
              " ('The acting was a bit lacking',\n",
              "  array([-3.5431213 , -0.03227618, -5.8646784 ], dtype=float32),\n",
              "  3),\n",
              " ('The film was creative and surprising',\n",
              "  array([-5.7396507 , -0.92527556, -0.5102227 ], dtype=float32),\n",
              "  5),\n",
              " ('Absolutely fantastic!',\n",
              "  array([-5.49177   , -5.7424636 , -0.00735433], dtype=float32),\n",
              "  5)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDi6X8R5LyDL",
        "colab_type": "text"
      },
      "source": [
        "#5.0 Save the model for later use\n",
        "\n",
        "1. just from checkpoint (which requires the architecture as the checkpoints only contain the parameters/weights)\n",
        "\n",
        "2. as the complete model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfw1log_LxO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trying to save the model, and read it in.\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}